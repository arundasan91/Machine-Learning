##A C++ Project to find the best features to train the algorithm on.

##Abstract
High quality Datasets are of prime importance for Machine Learning. So is choosing the best features from the dataset to train the program. The Project focuses on the selection of features from these huge dataset files according to its Entropy and Information Gain parameters. Entropy is the measure of impurity in the dataset and Information Gain is the parameter which decides which feature to choose. This problem is particular to Decision Tree algorithms. The more the Information Gain, the better. The Project will give user the ability to read a binary classified dataset. It will find the Entropy and Information Gain of various features and will select the best features among them so as to yield the best result out of our Decision Tree.

##Background
-	Machine learning is a type of artificial intelligence (AI) that provides computers with the ability to learn without being explicitly programmed. Machine learning focuses on the development of computer programs that can teach themselves to grow and change when exposed to new data.
-	Various machine learning applications are usually overwhelmed with a large number of features. The task of feature selection in these applications is to improve a performance criterion such as accuracy, but often also to minimize the cost associated in producing the features.
-	Feature selection reduces the dimensionality of feature space, removes redundant, irrelevant, or noisy data. It brings the immediate effects for application: speeding up a data mining algorithm, improving the data quality and thereof the performance of data mining.
-	Information Gain (IG) is an entropy-based feature evaluation method, widely used in the field of machine learning. It is defined as the amount of information provided by the feature for the Machine Learning algorithm
